{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:41:40.565775Z",
     "start_time": "2025-09-24T14:41:40.466818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the ARFF file\n",
    "data = arff.loadarff('Training Dataset.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "# Convert columns from bytes to integers if necessary\n",
    "df = df.astype(int)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop('Result', axis=1)\n",
    "y = df['Result']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ],
   "id": "66b38c7515647d99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (8844, 30)\n",
      "Testing set shape: (2211, 30)\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:41:48.002869Z",
     "start_time": "2025-09-24T14:41:40.569539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a pipeline that first scales the data, then runs the SVM\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', svm.SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# Create a dictionary of models to test\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "\t\"SVM\": svm.SVC(random_state=42),\n",
    "\t\"SVM with Scaler\": svm_pipeline,\n",
    "\t\"Knn\": KNeighborsClassifier(n_neighbors=3),\n",
    "\t\"Linear SVM\": LinearSVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Loop through and evaluate each model using cross-validation\n",
    "model_score = {}\n",
    "for name, model in models.items():\n",
    "    # Use 5-fold cross-validation and the F1-score as the metric\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n",
    "    model_score[name] = np.mean(scores)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"Scores: {scores}\")\n",
    "    print(f\"Mean F1-Score: {np.mean(scores):.4f}\")\n",
    "    print(f\"Std Deviation: {np.std(scores):.4f}\\n\")\n"
   ],
   "id": "6a21a1770aa27092",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression ---\n",
      "Scores: [0.93998991 0.93721748 0.93053473 0.929      0.9378474 ]\n",
      "Mean F1-Score: 0.9349\n",
      "Std Deviation: 0.0043\n",
      "\n",
      "--- Random Forest ---\n",
      "Scores: [0.9778002  0.9726997  0.96854718 0.96914517 0.97732997]\n",
      "Mean F1-Score: 0.9731\n",
      "Std Deviation: 0.0039\n",
      "\n",
      "--- Gradient Boosting ---\n",
      "Scores: [0.95753286 0.95481928 0.94842263 0.95233317 0.95866935]\n",
      "Mean F1-Score: 0.9544\n",
      "Std Deviation: 0.0037\n",
      "\n",
      "--- SVM ---\n",
      "Scores: [0.95582329 0.948      0.94474863 0.94984955 0.95420232]\n",
      "Mean F1-Score: 0.9505\n",
      "Std Deviation: 0.0040\n",
      "\n",
      "--- SVM with Scaler ---\n",
      "Scores: [0.96024157 0.95424837 0.94469357 0.95171026 0.95761857]\n",
      "Mean F1-Score: 0.9537\n",
      "Std Deviation: 0.0054\n",
      "\n",
      "--- Knn ---\n",
      "Scores: [0.95665323 0.94726166 0.93809763 0.94512195 0.94758065]\n",
      "Mean F1-Score: 0.9469\n",
      "Std Deviation: 0.0059\n",
      "\n",
      "--- Linear SVM ---\n",
      "Scores: [0.94046418 0.93762575 0.93020937 0.93113772 0.93743693]\n",
      "Mean F1-Score: 0.9354\n",
      "Std Deviation: 0.0040\n",
      "\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:41:48.099274Z",
     "start_time": "2025-09-24T14:41:48.095738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find the model with the best Mean F1-Score\n",
    "sorted_models = sorted(model_score.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nModels sorted by Mean F1-Score:\")\n",
    "for model, score in sorted_models:\n",
    "    print(f\"{model}: {score:.4f}\")\n",
    "\n",
    "best_model = max(model_score, key=model_score.get)\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Best score: {model_score[best_model]:.4f}\")\n"
   ],
   "id": "ca7cbba0de5f35bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models sorted by Mean F1-Score:\n",
      "Random Forest: 0.9731\n",
      "Gradient Boosting: 0.9544\n",
      "SVM with Scaler: 0.9537\n",
      "SVM: 0.9505\n",
      "Knn: 0.9469\n",
      "Linear SVM: 0.9354\n",
      "Logistic Regression: 0.9349\n",
      "Best model: Random Forest\n",
      "Best score: 0.9731\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:42:57.092369Z",
     "start_time": "2025-09-24T14:41:48.109441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluating the Results of GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [10, 20, 30, None],   # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10]   # Minimum number of samples to split a node\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1-score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# About 1 Mins"
   ],
   "id": "eb6e4da17da35df9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best cross-validation F1-score: 0.9731\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:43:10.864743Z",
     "start_time": "2025-09-24T14:43:10.838144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Validation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get the best estimator from the grid search\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(\"\\n--- Final Model Evaluation on Test Set ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate (-1)', 'Phishing (1)']))\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ],
   "id": "ef4137e081996354",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Model Evaluation on Test Set ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Legitimate (-1)       0.98      0.96      0.97       980\n",
      "   Phishing (1)       0.97      0.98      0.98      1231\n",
      "\n",
      "       accuracy                           0.98      2211\n",
      "      macro avg       0.98      0.97      0.98      2211\n",
      "   weighted avg       0.98      0.98      0.98      2211\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 945   35]\n",
      " [  19 1212]]\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building a Complete ML Pipeline for Random Forest Model\n",
    "\n",
    "Based on your existing code, I'll create a comprehensive pipeline that includes preprocessing, hyperparameter tuning, and model evaluation."
   ],
   "id": "7d0063d1105e869e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:43:23.960513Z",
     "start_time": "2025-09-24T14:43:23.957175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import joblib\n",
    "\n",
    "# Create a comprehensive pipeline with preprocessing and model\n",
    "rf_pipeline = Pipeline([\n",
    "\t('preprocessor', StandardScaler()),\n",
    "\t('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Random Forest Pipeline created successfully\")\n",
    "print(f\"Pipeline steps: {rf_pipeline.steps}\")"
   ],
   "id": "31253eae1197a53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Pipeline created successfully\n",
      "Pipeline steps: [('preprocessor', StandardScaler()), ('classifier', RandomForestClassifier(random_state=42))]\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:43:26.579917Z",
     "start_time": "2025-09-24T14:43:26.576476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define hyperparameter grid for pipeline\n",
    "pipeline_param_grid = {\n",
    "\t'classifier__n_estimators': [100, 200, 300],\n",
    "\t'classifier__max_depth': [10, 20, 30, None],\n",
    "\t'classifier__min_samples_split': [2, 5, 10],\n",
    "\t'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV with the pipeline\n",
    "pipeline_grid_search = GridSearchCV(\n",
    "\testimator=rf_pipeline,\n",
    "\tparam_grid=pipeline_param_grid,\n",
    "\tcv=5,\n",
    "\tscoring='f1',\n",
    "\tverbose=1\n",
    ")\n",
    "\n",
    "print(\"Pipeline GridSearchCV object created\")\n",
    "print(\n",
    "\tf\"Total combinations to test: {len(pipeline_param_grid['classifier__n_estimators']) * len(pipeline_param_grid['classifier__max_depth']) * len(pipeline_param_grid['classifier__min_samples_split']) * len(pipeline_param_grid['classifier__min_samples_leaf'])}\")"
   ],
   "id": "b6e9b65ab140a795",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline GridSearchCV object created\n",
      "Total combinations to test: 108\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:46:50.847231Z",
     "start_time": "2025-09-24T14:43:29.046381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fit the pipeline with hyperparameter tuning\n",
    "print(\"Training pipeline with hyperparameter tuning...\")\n",
    "pipeline_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best pipeline parameters: {pipeline_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1-score: {pipeline_grid_search.best_score_:.4f}\")"
   ],
   "id": "23bed7855ef449bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pipeline with hyperparameter tuning...\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best pipeline parameters: {'classifier__max_depth': 20, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 300}\n",
      "Best cross-validation F1-score: 0.9731\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:46:55.921350Z",
     "start_time": "2025-09-24T14:46:55.865092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the best pipeline\n",
    "best_pipeline = pipeline_grid_search.best_estimator_\n",
    "\n",
    "# Make predictions using the complete pipeline\n",
    "pipeline_predictions = best_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate pipeline performance\n",
    "print(\"=== Pipeline Performance on Test Set ===\")\n",
    "print(classification_report(y_test, pipeline_predictions, target_names=['Legitimate (-1)', 'Phishing (1)']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, pipeline_predictions))\n",
    "\n",
    "# Calculate F1 score\n",
    "pipeline_f1_score = f1_score(y_test, pipeline_predictions)\n",
    "print(f\"\\nPipeline F1-Score: {pipeline_f1_score:.4f}\")"
   ],
   "id": "fcd0f9faee99c575",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pipeline Performance on Test Set ===\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Legitimate (-1)       0.98      0.96      0.97       980\n",
      "   Phishing (1)       0.97      0.98      0.98      1231\n",
      "\n",
      "       accuracy                           0.98      2211\n",
      "      macro avg       0.98      0.97      0.97      2211\n",
      "   weighted avg       0.98      0.98      0.98      2211\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 945   35]\n",
      " [  20 1211]]\n",
      "\n",
      "Pipeline F1-Score: 0.9778\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:47:17.058975Z",
     "start_time": "2025-09-24T14:47:17.055407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the trained pipeline\n",
    "#joblib.dump(best_pipeline, 'best_rf_pipeline.pkl')\n",
    "\n",
    "\n",
    "# Create a prediction function using the pipeline\n",
    "def predict_phishing(features):\n",
    "\t\"\"\"\n",
    "    Predict if a URL is phishing using the trained pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    features: array-like, shape (n_features,) or (n_samples, n_features)\n",
    "    \n",
    "    Returns:\n",
    "    predictions: array, predicted labels (-1 for legitimate, 1 for phishing)\n",
    "    \"\"\"\n",
    "\treturn best_pipeline.predict(features)\n",
    "\n",
    "\n",
    "# Create a prediction probability function\n",
    "def predict_phishing_proba(features):\n",
    "\t\"\"\"\n",
    "    Get prediction probabilities using the trained pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    features: array-like, shape (n_features,) or (n_samples, n_features)\n",
    "    \n",
    "    Returns:\n",
    "    probabilities: array, shape (n_samples, 2) - probabilities for each class\n",
    "    \"\"\"\n",
    "\treturn best_pipeline.predict_proba(features)\n",
    "\n",
    "\n",
    "print(\"Pipeline saved as 'best_rf_pipeline.pkl'\")\n",
    "print(\"Prediction functions created successfully\")"
   ],
   "id": "b42c98cd43d60dcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline saved as 'best_rf_pipeline.pkl'\n",
      "Prediction functions created successfully\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:47:32.012075Z",
     "start_time": "2025-09-24T14:47:31.998485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compare pipeline performance with previous best model\n",
    "print(\"=== Model Comparison ===\")\n",
    "print(f\"Previous best Random Forest F1-score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Pipeline Random Forest F1-score: {pipeline_f1_score:.4f}\")\n",
    "\n",
    "improvement = pipeline_f1_score - f1_score(y_test, y_pred)\n",
    "print(f\"Performance improvement: {improvement:.4f}\")\n",
    "\n",
    "# Display feature importance from the pipeline\n",
    "feature_importance = best_pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a dataframe for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "\t'feature': feature_names,\n",
    "\t'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n=== Top 10 Most Important Features ===\")\n",
    "print(importance_df.head(10))"
   ],
   "id": "809cbc8adc48dd99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Comparison ===\n",
      "Previous best Random Forest F1-score: 0.9782\n",
      "Pipeline Random Forest F1-score: 0.9778\n",
      "Performance improvement: -0.0004\n",
      "\n",
      "=== Top 10 Most Important Features ===\n",
      "                        feature  importance\n",
      "7                SSLfinal_State    0.322322\n",
      "13                URL_of_Anchor    0.254405\n",
      "25                  web_traffic    0.068522\n",
      "6             having_Sub_Domain    0.062755\n",
      "14                Links_in_tags    0.042478\n",
      "5                 Prefix_Suffix    0.041636\n",
      "15                          SFH    0.019660\n",
      "12                  Request_URL    0.019315\n",
      "28       Links_pointing_to_page    0.019310\n",
      "8   Domain_registeration_length    0.016832\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:47:35.829675Z",
     "start_time": "2025-09-24T14:47:35.825140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a complete ML pipeline class for reusability\n",
    "class PhishingDetectionPipeline:\n",
    "\tdef __init__(self):\n",
    "\t\tself.pipeline = None\n",
    "\t\tself.is_trained = False\n",
    "\n",
    "\tdef build_pipeline(self):\n",
    "\t\t\"\"\"Build the ML pipeline\"\"\"\n",
    "\t\tself.pipeline = Pipeline([\n",
    "\t\t\t('preprocessor', StandardScaler()),\n",
    "\t\t\t('classifier', RandomForestClassifier(random_state=42))\n",
    "\t\t])\n",
    "\t\treturn self.pipeline\n",
    "\n",
    "\tdef train(self, X_train, y_train, hyperparameter_tuning=True):\n",
    "\t\t\"\"\"Train the pipeline\"\"\"\n",
    "\t\tif self.pipeline is None:\n",
    "\t\t\tself.build_pipeline()\n",
    "\n",
    "\t\tif hyperparameter_tuning:\n",
    "\t\t\tparam_grid = {\n",
    "\t\t\t\t'classifier__n_estimators': [100, 200, 300],\n",
    "\t\t\t\t'classifier__max_depth': [10, 20, 30, None],\n",
    "\t\t\t\t'classifier__min_samples_split': [2, 5, 10]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tgrid_search = GridSearchCV(\n",
    "\t\t\t\tself.pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1\n",
    "\t\t\t)\n",
    "\t\t\tgrid_search.fit(X_train, y_train)\n",
    "\t\t\tself.pipeline = grid_search.best_estimator_\n",
    "\t\t\tself.best_params = grid_search.best_params_\n",
    "\t\t\tself.best_score = grid_search.best_score_\n",
    "\t\telse:\n",
    "\t\t\tself.pipeline.fit(X_train, y_train)\n",
    "\n",
    "\t\tself.is_trained = True\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\t\"\"\"Make predictions\"\"\"\n",
    "\t\tif not self.is_trained:\n",
    "\t\t\traise ValueError(\"Pipeline must be trained before making predictions\")\n",
    "\t\treturn self.pipeline.predict(X)\n",
    "\n",
    "\tdef predict_proba(self, X):\n",
    "\t\t\"\"\"Get prediction probabilities\"\"\"\n",
    "\t\tif not self.is_trained:\n",
    "\t\t\traise ValueError(\"Pipeline must be trained before making predictions\")\n",
    "\t\treturn self.pipeline.predict_proba(X)\n",
    "\n",
    "\tdef save_model(self, filename):\n",
    "\t\t\"\"\"Save the trained pipeline\"\"\"\n",
    "\t\tif not self.is_trained:\n",
    "\t\t\traise ValueError(\"Pipeline must be trained before saving\")\n",
    "\t\tjoblib.dump(self.pipeline, filename)\n",
    "\n",
    "\tdef load_model(self, filename):\n",
    "\t\t\"\"\"Load a trained pipeline\"\"\"\n",
    "\t\tself.pipeline = joblib.load(filename)\n",
    "\t\tself.is_trained = True\n",
    "\n",
    "\n",
    "# Example usage of the pipeline class\n",
    "phishing_pipeline = PhishingDetectionPipeline()\n",
    "print(\"PhishingDetectionPipeline class created successfully\")\n"
   ],
   "id": "914741801a381d3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhishingDetectionPipeline class created successfully\n"
     ]
    }
   ],
   "execution_count": 73
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
